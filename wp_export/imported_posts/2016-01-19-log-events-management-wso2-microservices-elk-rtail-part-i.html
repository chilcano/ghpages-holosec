---
layout: post
title: 'Log Events Management in WSO2 (Micro)services: ELK & rTail (Part I)'
date: 2016-01-19 17:49:31.000000000 +01:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Microservices
- SOA
tags:
- Docker
- ELK
- Logging
- rTail
- Vagrant
- Wiremock
- WSO2
meta:
  _wpcom_is_markdown: '1'
  _edit_last: '578869'
  _oembed_05f87f7fdb8e25a7d1265e3ff1b076ae: "{{unknown}}"
  geo_public: '0'
  _oembed_ebc21c5371f3ce6cac236e99f5440ab4: "{{unknown}}"
  _oembed_670837f2c42bada9212dcad97e300749: "{{unknown}}"
  _oembed_86d74a69af3db2c4f532a950e83a6557: "{{unknown}}"
  _oembed_fe0927127f443d79765a25775e5c4693: "{{unknown}}"
  _publicize_done_external: a:1:{s:7:"twitter";a:1:{i:13849;s:54:"https://twitter.com/Chilcano/status/689490007510282242";}}
  _publicize_job_id: '18910974427'
  _oembed_76c6da8e3187d419b6dcc51d42a68a11: "{{unknown}}"
  publicize_google_plus_url: https://plus.google.com/+RogerCarhuatocto/posts/RTcrUusZdWQ
  _publicize_done_5110107: '1'
  _wpas_done_5053089: '1'
  publicize_linkedin_url: https://www.linkedin.com/updates?discuss=&scope=6985267&stype=M&topic=6095255618598289408&type=U&a=2xz7
  _publicize_done_5110110: '1'
  _wpas_done_5053092: '1'
  _publicize_done_17477: '1'
  _wpas_done_13849: '1'
  publicize_twitter_user: Chilcano
  _oembed_70fa7f29806226940484ce9b70b4121c: "{{unknown}}"
  _oembed_15b61e82ab1b94b98ac770b10928dc5e: "{{unknown}}"
  _oembed_868eda38880d53a751ae54b8b5fd7e48: "{{unknown}}"
  _oembed_ba5b1f9ac5401566c2c26ed9077a7525: "{{unknown}}"
  _oembed_d7abb0c4ab86146e5d6b14bd5145fa84: "{{unknown}}"
  _oembed_9df507a2f96735a854630464cbf0c59e: "{{unknown}}"
author:
  login: rcarhuatocto
  email: roger@intix.info
  display_name: Roger Carhuatocto
  first_name: ''
  last_name: ''
permalink: "/2016/01/19/log-events-management-wso2-microservices-elk-rtail-part-i/"
---
<p>The log event management is a task very important when working with (Micro)services. If you collect-store-index all logs, then you will be able to create your business metrics (KPI). You only must understand what by collecting logs you have to use special tools with technical special requirements:<br />
* Huge amout of logs could transform in your <code>BigData</code> asset.<br />
* You have to collect and query your logs in real time if you think your Application is critical.<br />
* You have to <code>manage</code> in agile way the life cycle of your data (your logs), they are an asset very important for your Organization.</p>
<p><img src="{{ site.baseurl }}/assets/blog-chilcano-logs-wso2-docker-elk-rtail-0-architecture.png" alt="Kibana - Viewing WSO2 and Wiremock raw log events" title="Collecting WSO2 and Wiremock log events with ELK and rTail" />Collecting WSO2 and Wiremock log events with ELK and rTail</p>
<p><!-- more --></p>
<p>You should be agile, for this reason, <a href="https://www.elastic.co">Elasticsearch-Logstash-Kibana</a> is the perfect Stack to do that in an agile way.<br />
Really, there are many tools out there, opensource, commercial, on-cloud, such as log.io, Clarity, rTail, Tailon, frontail, etc. In my opinion, for a development environment, the most simple, fresh and lightweight tool is rTail (http://rtail.org), with rTail I can collect different log files, track and visualize them from a Browser in real time. rTail is very easy to use, just install NodeJS and deploy rTail application and you will be ready to send any type of traces to Browser directly avoiding store/persist logs, index and parse/filter them.<br />
Well, this first blog post I will explain how to use ELK to collect, store and view the different log event of WSO2 ESB, API Manager, DSS, GREG and Wiremock.</p>
<h2>Part I: ELK (Elasticsearch, Logstash, Kibana)</h2>
<h3>1. Starting with ELK Docker Container</h3>
<p><strong>1) Prepare the ELK container</strong></p>
<p>We gonna use an existing Docker Image with ELK created by <a href="https://github.com/spujadas">Sébastien Pujadas</a> <code>(http://elk-docker.readthedocs.org)</code> previously configured ready to be used. This Docker Image contains:<br />
- Elasticsearch (version 2.1.1)<br />
- Logstash (version 2.1.1)<br />
- Kibana (version 4.3.1)</p>
<p>Then, let's do it. Start the Docker daemon and login Docker Hub:</p>
<p>[code lang=bash]<br />
$ docker login<br />
Username (chilcano):<br />
WARNING: login credentials saved in /Users/Chilcano/.docker/config.json<br />
Login Succeeded</p>
<p>$ docker pull sebp/elk<br />
Using default tag: latest<br />
latest: Pulling from sebp/elk<br />
de9c48daf08c: Pull complete<br />
...<br />
96f071b7a8e2: Pull complete<br />
Digest: sha256:ce7b3a1dfe285d1d9b862905bf0ee6df951f1a035120b92af71280217b6f3422<br />
Status: Downloaded newer image for sebp/elk:latest<br />
[/code]</p>
<p><strong>2) Run the container</strong></p>
<p>[code lang=bash]<br />
$ docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -p 5000:5000 -it --name elk sebp/elk<br />
 * Starting Elasticsearch Server<br />
 sysctl: setting key &quot;vm.max_map_count&quot;: Read-only file system logstash started.                                                                                                                              [ OK ]</p>
<p>waiting for Elasticsearch to be up (1/30)<br />
waiting for Elasticsearch to be up (2/30)<br />
waiting for Elasticsearch to be up (3/30)<br />
waiting for Elasticsearch to be up (4/30)<br />
waiting for Elasticsearch to be up (5/30)<br />
waiting for Elasticsearch to be up (6/30)<br />
waiting for Elasticsearch to be up (7/30)<br />
 * Starting Kibana4                                                                                                                                                                                           [ OK ]<br />
[2016-01-14 08:48:01,215][INFO ][node                     ] [Zero] initialized<br />
[2016-01-14 08:48:01,216][INFO ][node                     ] [Zero] starting ...<br />
[2016-01-14 08:48:01,259][WARN ][common.network           ] [Zero] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}<br />
[2016-01-14 08:48:01,259][INFO ][transport                ] [Zero] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}<br />
[2016-01-14 08:48:01,297][INFO ][discovery                ] [Zero] elasticsearch/vyxjAXavQbOnh7uA7HO_7g<br />
[2016-01-14 08:48:04,341][INFO ][cluster.service          ] [Zero] new_master {Zero}{vyxjAXavQbOnh7uA7HO_7g}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)<br />
[2016-01-14 08:48:04,365][WARN ][common.network           ] [Zero] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}<br />
[2016-01-14 08:48:04,365][INFO ][http                     ] [Zero] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}<br />
[2016-01-14 08:48:04,365][INFO ][node                     ] [Zero] started<br />
[2016-01-14 08:48:04,384][INFO ][gateway                  ] [Zero] recovered [0] indices into cluster_state<br />
[2016-01-14 08:48:13,180][INFO ][cluster.metadata         ] [Zero] [.kibana] creating index, cause [api], templates [], shards [1]/[1], mappings [config]<br />
[/code]</p>
<p><strong>3) Check the status of ELK in the running container</strong></p>
<p>In other Terminal/Shell execute the next:</p>
<p>[code lang=bash]<br />
$ docker-machine ls<br />
NAME           ACTIVE   DRIVER       STATE     URL                         SWARM   ERRORS<br />
default        -        virtualbox   Running   tcp://192.168.99.100:2376<br />
machine-dev    -        virtualbox   Stopped<br />
machine-test   -        virtualbox   Stopped</p>
<p>$ docker-machine env default<br />
export DOCKER_TLS_VERIFY=&quot;1&quot;<br />
export DOCKER_HOST=&quot;tcp://192.168.99.100:2376&quot;<br />
export DOCKER_CERT_PATH=&quot;/Users/Chilcano/.docker/machine/machines/default&quot;<br />
export DOCKER_MACHINE_NAME=&quot;default&quot;<br />
# Run this command to configure your shell:<br />
# eval &quot;$(docker-machine env default)&quot;</p>
<p>$ eval &quot;$(docker-machine env default)&quot;</p>
<p>$ docker ps<br />
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                                                      NAMES<br />
788b97b04e9b        sebp/elk            &quot;/usr/local/bin/start&quot;   9 minutes ago       Up 9 minutes        0.0.0.0:5000-&gt;5000/tcp, 0.0.0.0:5044-&gt;5044/tcp, 0.0.0.0:5601-&gt;5601/tcp, 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp   elk<br />
[/code]</p>
<p>The ports opened are:</p>
<ul>
<li>5601 (Kibana web interface).</li>
<li>9200 (Elasticsearch JSON interface).</li>
<li>5044 (Logstash Beats interface, receives logs from Beats such as Filebeat).</li>
<li>5000 (Logstash Lumberjack interface, receives logs from Logstash forwarders).</li>
</ul>
<p><strong>4) Check Elasticsearch server</strong></p>
<p>[code lang=bash]<br />
$ curl -H &quot;User-Agent: Mozilla&quot; -H &quot;Origin: http://example.com&quot; -i 192.168.99.100:9200<br />
HTTP/1.1 200 OK<br />
Content-Type: application/json; charset=UTF-8<br />
Content-Length: 313</p>
<p>{<br />
  &quot;name&quot; : &quot;Zero&quot;,<br />
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,<br />
  &quot;version&quot; : {<br />
    &quot;number&quot; : &quot;2.1.1&quot;,<br />
    &quot;build_hash&quot; : &quot;40e2c53a6b6c2972b3d13846e450e66f4375bd71&quot;,<br />
    &quot;build_timestamp&quot; : &quot;2015-12-15T13:05:55Z&quot;,<br />
    &quot;build_snapshot&quot; : false,<br />
    &quot;lucene_version&quot; : &quot;5.3.1&quot;<br />
  },<br />
  &quot;tagline&quot; : &quot;You Know, for Search&quot;<br />
}<br />
[/code]</p>
<p>And if you want to stop and start again the container, just execute the next:</p>
<p>[code lang=bash]<br />
$ docker stop elk<br />
elk</p>
<p>$ docker start elk<br />
elk<br />
[/code]</p>
<p><strong>5) Sending a dummy logs to ELK</strong></p>
<p>In another terminal execute the next:</p>
<p>[code lang=bash]<br />
$ docker exec -it elk /bin/bash</p>
<p>$ docker exec -it elk /bin/bash<br />
root@788b97b04e9b:/# /opt/logstash/bin/logstash -e &#039;input { stdin { } } output { elasticsearch { hosts =&gt; [&quot;localhost&quot;] } }&#039;<br />
Settings: Default filter workers: 1<br />
Logstash startup completed<br />
Hola Chilcano!!<br />
^CSIGINT received. Shutting down the pipeline. {:level=&gt;:warn}</p>
<p>Logstash shutdown completed<br />
root@788b97b04e9b:/#<br />
[/code]</p>
<p>In other terminal using cURL or from a browser:</p>
<p>[code lang=bash]<br />
$ curl http://192.168.99.100:9200/_search?pretty<br />
{<br />
  &quot;took&quot; : 5,<br />
  &quot;timed_out&quot; : false,<br />
  &quot;_shards&quot; : {<br />
    &quot;total&quot; : 6,<br />
    &quot;successful&quot; : 6,<br />
    &quot;failed&quot; : 0<br />
  },<br />
  &quot;hits&quot; : {<br />
    &quot;total&quot; : 2,<br />
    &quot;max_score&quot; : 1.0,<br />
    &quot;hits&quot; : [ {<br />
      &quot;_index&quot; : &quot;.kibana&quot;,<br />
      &quot;_type&quot; : &quot;config&quot;,<br />
      &quot;_id&quot; : &quot;4.3.1&quot;,<br />
      &quot;_score&quot; : 1.0,<br />
      &quot;_source&quot;:{&quot;buildNum&quot;:9517}<br />
    }, {<br />
      &quot;_index&quot; : &quot;logstash-2016.01.14&quot;,<br />
      &quot;_type&quot; : &quot;logs&quot;,<br />
      &quot;_id&quot; : &quot;AVJA8Na_dQWajjStHmJN&quot;,<br />
      &quot;_score&quot; : 1.0,<br />
      &quot;_source&quot;:{&quot;message&quot;:&quot;Hola Chilcano!!&quot;,&quot;@version&quot;:&quot;1&quot;,&quot;@timestamp&quot;:&quot;2016-01-14T16:21:11.231Z&quot;,&quot;host&quot;:&quot;788b97b04e9b&quot;}<br />
    } ]<br />
  }<br />
}<br />
[/code]</p>
<p>... and from the Kibana web console (<code>http://192.168.99.100:5601</code>) to view the incoming dummy log event. Before configure Kibana by creating a <code>Index Pattern</code> with <code>logstash-*</code> and <code>Time-field name: @timestamp</code>, as shown below:</p>
<p><em>Kibana - Creating a Index Pattern</em><br />
<img src="{{ site.baseurl }}/assets/blog-chilcano-logs-wso2-docker-elk-rtail-1-kibana.png" alt="Kibana - Creating a Index Pattern" title="Index Pattern with logstash-*" /><br />
<em>Kibana - Selecting the field of the new Index Pattern</em><br />
<img src="{{ site.baseurl }}/assets/blog-chilcano-logs-wso2-docker-elk-rtail-2-kibana-dummy-logs.png" alt="Kibana - Selecting the field of the new Index Pattern" title="The field for the new Index Pattern" /><br />
<em>Kibana - Viewing a dummy log event</em><br />
<img src="{{ site.baseurl }}/assets/blog-chilcano-logs-wso2-docker-elk-rtail-3-kibana-dummy-logs-discovery.png" alt="Kibana - Viewing a dummy log event" title="Viewing a dummy log event" /></p>
<h3>2. Sending WSO2 logs to the ELK Docker container</h3>
<p>For this part I will use a Vagrant box with several WSO2 products pre-installed and pre-configured. I explained how to use It in this blog, for further details check that post <a href="https://holisticsecurity.wordpress.com/2015/11/11/creating-a-vm-with-wso2-servers-for-development">https://holisticsecurity.wordpress.com/2015/11/11/creating-a-vm-with-wso2-servers-for-development</a>.</p>
<blockquote><p>
  Filebeat is a lightweight, open source shipper for log file data. As the next-generation<br />
  Logstash Forwarder, Filebeat tails logs and quickly sends this information to Logstash<br />
  for further parsing and enrichment or to Elasticsearch for centralized storage and analysis.<br />
  <em><a href="https://www.elastic.co/products/beats/filebeat">www.elastic.co/products/beats/filebeat</a></em>
</p></blockquote>
<p><strong>1) Start the WSO2 Vagrant box</strong></p>
<p>[code lang=bash]<br />
$ git clone https://github.com/chilcano/box-vagrant-wso2-dev-srv.git</p>
<p>$ cd box-vagrant-wso2-dev-srv</p>
<p>$ vagrant up<br />
[/code]</p>
<p>[code lang=bash]<br />
$ vagrant ssh<br />
Welcome to Ubuntu 14.04.3 LTS (GNU/Linux 3.13.0-67-generic i686)</p>
<p> * Documentation:  https://help.ubuntu.com/</p>
<p>  System information as of Tue Jan 12 10:41:00 UTC 2016</p>
<p>  System load:  1.76               Processes:           108<br />
  Usage of /:   33.9% of 39.34GB   Users logged in:     0<br />
  Memory usage: 42%                IP address for eth0: 10.0.2.15<br />
  Swap usage:   0%                 IP address for eth1: 192.168.11.20</p>
<p>  Graph this data and manage this system at:<br />
    https://landscape.canonical.com/</p>
<p>  Get cloud support with Ubuntu Advantage Cloud Guest:<br />
    http://www.ubuntu.com/business/services/cloud</p>
<p>9 packages can be updated.<br />
7 updates are security updates.</p>
<p>Last login: Tue Jan 12 10:41:01 2016 from 10.0.2.2</p>
<p>[11:04 AM]-[vagrant@wso2-dev-srv-01]-[~]<br />
$ cd /opt/</p>
<p>[11:04 AM]-[vagrant@wso2-dev-srv-01]-[/opt]<br />
$ ll<br />
total 40<br />
drwxr-xr-x 11 vagrant vagrant 4096 Nov 30 23:43 activemq/<br />
drwxr-xr-x  2 vagrant vagrant 4096 Jan 12 10:39 rtail/<br />
drwxr-xr-x  9 root    root    4096 Nov 18 18:18 VBoxGuestAdditions-4.3.10/<br />
drwxr-xr-x  4 vagrant vagrant 4096 Nov 30 15:41 wiremock/<br />
drwxr-xr-x 15 vagrant vagrant 4096 Nov 30 15:41 wso2am02a/<br />
drwxr-xr-x 10 vagrant vagrant 4096 Jan 12 10:40 wso2dss01a/<br />
drwxr-xr-x 11 vagrant vagrant 4096 Nov 23 12:44 wso2esb01a/<br />
drwxr-xr-x 11 vagrant vagrant 4096 Nov 23 12:45 wso2esb02a/<br />
drwxr-xr-x 10 vagrant vagrant 4096 Nov 30 23:14 wso2esb490/<br />
drwxr-xr-x 13 vagrant vagrant 4096 Jan 12 10:41 wso2greg01a/<br />
[/code]</p>
<p>If you already are running this Vagrant box with WSO2 ESB, WSO2 API Manager, WSO2 DSS, Wiremock, etc. then the next step is configure It to send the different generated logs to ELK Docker Container.<br />
In the documentation explains that It will be used <a href="https://www.elastic.co/products/beats/filebeat">Filebeat</a>, for that we have to install and configure Filebeat in this Vagrant box.</p>
<p><strong>2) Install Filebeat into Vagrant box</strong></p>
<p>[code lang=bash]<br />
$ sudo curl -L -O https://download.elastic.co/beats/filebeat/filebeat_1.0.1_i386.deb<br />
$ sudo dpkg -i filebeat_1.0.1_i386.deb<br />
$ sudo rm filebeat_1.0.1_i386.deb<br />
[/code]</p>
<p><strong>3) Configure Filebeat to forward WSO2 logs to ELK Docker Container</strong></p>
<p>[code lang=bash]<br />
$ ll /etc/filebeat/<br />
total 20<br />
-rw-r--r-- 1 root root   814 Dec 17 13:26 filebeat.template.json<br />
-rw-r--r-- 1 root root 14541 Dec 17 13:26 filebeat.yml<br />
[/code]</p>
<p>[code lang=bash]<br />
$ sudo nano filebeat.yml</p>
<p>output:<br />
  logstash:<br />
    enabled: true<br />
    hosts: [&quot;elk-docker:5044&quot;]<br />
    timeout: 15<br />
    tls:<br />
      certificate_authorities: [&quot;/etc/pki/tls/certs/logstash-beats.crt&quot;]</p>
<p>filebeat:<br />
  prospectors:<br />
    -<br />
      paths:<br />
        - /opt/wso2esb01a/repository/logs/wso2carbon.log<br />
        - /opt/wso2esb02a/repository/logs/wso2carbon.log<br />
        - /opt/wso2am02a/repository/logs/wso2carbon.log<br />
        - /opt/wso2dss01a/repository/logs/wso2carbon.log<br />
        - /opt/wso2greg01a/repository/logs/wso2carbon.log<br />
      document_type: log<br />
    -<br />
      paths:<br />
        - /opt/wiremock/wiremock.log<br />
      document_type: log<br />
logging:<br />
  level: warning<br />
  to_files: true<br />
  to_syslog: false</p>
<p>  files:<br />
    path: /var/log/filebeat<br />
    name: filebeat.log<br />
    keepfiles: 7</p>
<p>[/code]</p>
<p>Where <code>elk-docker:5044</code> is the hostname for the <code>192.168.99.100</code> added to <code>/etc/hosts</code> of Vagrant box.</p>
<p>Copy the <code>/etc/pki/tls/certs/logstash-beats.crt</code> file from Logstash Beats input plugin (ELK Docker Container) into <code>/etc/pki/tls/certs/logstash-beats.crt</code> (Vagrant box).</p>
<p>[code lang=text]<br />
$ sudo mkdir -p /etc/pki/tls/certs/<br />
$ cd /etc/pki/tls/certs/<br />
$ sudo wget https://raw.githubusercontent.com/spujadas/elk-docker/master/nginx-filebeat/logstash-beats.crt</p>
<p>### or from ELK container SCP the *.crt file to Vagrant (open 2222 port).<br />
$ scp -P 2222 /etc/pki/tls/certs/logstash-beats.crt vagrant@192.168.1.43:/etc/pki/tls/certs/logstash-beats.crt<br />
[/code]</p>
<p>To avoid the below error, to update the <code>host</code> with a hostname (not IP address) as <code>elk-docker</code> in <code>/etc/filebeat/filebeat.yml</code> file, after that update also <code>/etc/hosts</code> with the appropiate IP Address.</p>
<p>[code lang=bash]<br />
$ sudo /etc/init.d/filebeat start<br />
2016/01/15 17:30:22.910725 transport.go:125: ERR SSL client failed to connect with: x509: cannot validate certificate for 192.168.99.100 because it doesn&#039;t contain any IP SANs<br />
[/code]</p>
<p><strong>4) Loading the Index Template in Elasticsearch</strong></p>
<p>Before starting Filebeat for the first time, run this command to load the default index template in Elasticsearch from the Vagrant box:</p>
<p>[code lang=bash]<br />
$ curl -XPUT &#039;http://192.168.99.100:9200/_template/filebeat?pretty&#039; -d@/etc/filebeat/filebeat.template.json<br />
{<br />
  &quot;acknowledged&quot; : true<br />
}<br />
[/code]</p>
<p><strong>5) Start Filebeat daemon</strong></p>
<p>[code lang=bash]<br />
$ sudo /etc/init.d/filebeat restart<br />
 * Restarting Sends log files to Logstash or directly to Elasticsearch. filebeat<br />
2016/01/19 10:55:36.335563 beat.go:97: DBG  Initializing output plugins<br />
2016/01/19 10:55:36.337701 geolite.go:24: INFO GeoIP disabled: No paths were set under output.geoip.paths<br />
2016/01/19 10:55:36.406510 outputs.go:111: INFO Activated logstash as output plugin.<br />
2016/01/19 10:55:36.406694 publish.go:198: DBG  create output worker: 0x0, 0x0<br />
2016/01/19 10:55:36.406901 publish.go:235: DBG  No output is defined to store the topology. The server fields might not be filled.<br />
2016/01/19 10:55:36.407160 publish.go:249: INFO Publisher name: wso2-dev-srv-01<br />
2016/01/19 10:55:36.407587 async.go:95: DBG  create bulk processing worker (interval=1s, bulk size=200)<br />
2016/01/19 10:55:36.408164 beat.go:107: INFO Init Beat: filebeat; Version: 1.0.1<br />
   ...done.<br />
[/code]</p>
<p>or if You have created <code>filebeat.yml</code> in a different folder.</p>
<p>[code lang=bash]<br />
$ sudo ./filebeat -e -c /myfolder/filebeat.yml<br />
[/code]</p>
<p><strong>6) Check if Filebeat is running</strong></p>
<p>I have created a <code>filebeat.yml</code> file with the logs section enabled. That's suitable to verify of everything is OK.</p>
<p>[code lang=bash]<br />
$ sudo tail -10000f /var/log/filebeat/filebeat.log<br />
...<br />
2016-01-19T10:56:16Z DBG  Start next scan<br />
2016-01-19T10:56:16Z DBG  scan path /opt/wso2esb01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Check file for harvesting: /opt/wso2esb01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Update existing file for harvesting: /opt/wso2esb01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Not harvesting, file didn&#039;t change: /opt/wso2esb01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  scan path /opt/wso2esb02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Check file for harvesting: /opt/wso2esb02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Update existing file for harvesting: /opt/wso2esb02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Not harvesting, file didn&#039;t change: /opt/wso2esb02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  scan path /opt/wso2am02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Check file for harvesting: /opt/wso2am02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Update existing file for harvesting: /opt/wso2am02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Not harvesting, file didn&#039;t change: /opt/wso2am02a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  scan path /opt/wso2dss01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Check file for harvesting: /opt/wso2dss01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Update existing file for harvesting: /opt/wso2dss01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Not harvesting, file didn&#039;t change: /opt/wso2dss01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  scan path /opt/wso2greg01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Check file for harvesting: /opt/wso2greg01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Update existing file for harvesting: /opt/wso2greg01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:16Z DBG  Not harvesting, file didn&#039;t change: /opt/wso2greg01a/repository/logs/wso2carbon.log<br />
2016-01-19T10:56:24Z DBG  Flushing spooler because of timemout. Events flushed: 2<br />
2016-01-19T10:56:24Z DBG  send event<br />
2016-01-19T10:56:24Z DBG  Start Preprocessing<br />
2016-01-19T10:56:24Z DBG  Publish: {<br />
  &quot;@timestamp&quot;: &quot;2016-01-19T10:56:21.533Z&quot;,<br />
  &quot;beat&quot;: {<br />
    &quot;hostname&quot;: &quot;wso2-dev-srv-01&quot;,<br />
    &quot;name&quot;: &quot;wso2-dev-srv-01&quot;<br />
  },<br />
  &quot;count&quot;: 1,<br />
  &quot;fields&quot;: null,<br />
  &quot;input_type&quot;: &quot;log&quot;,<br />
  &quot;message&quot;: &quot;&quot;,<br />
  &quot;offset&quot;: 973,<br />
  &quot;source&quot;: &quot;/opt/wiremock/wiremock.log&quot;,<br />
  &quot;type&quot;: &quot;log&quot;<br />
}<br />
2016-01-19T10:56:24Z DBG  Publish: {<br />
  &quot;@timestamp&quot;: &quot;2016-01-19T10:56:21.533Z&quot;,<br />
  &quot;beat&quot;: {<br />
    &quot;hostname&quot;: &quot;wso2-dev-srv-01&quot;,<br />
    &quot;name&quot;: &quot;wso2-dev-srv-01&quot;<br />
  },<br />
  &quot;count&quot;: 1,<br />
  &quot;fields&quot;: null,<br />
  &quot;input_type&quot;: &quot;log&quot;,<br />
  &quot;message&quot;: &quot;&quot;,<br />
  &quot;offset&quot;: 974,<br />
  &quot;source&quot;: &quot;/opt/wiremock/wiremock.log&quot;,<br />
  &quot;type&quot;: &quot;log&quot;<br />
}<br />
2016-01-19T10:56:24Z DBG  Forward preprocessed events<br />
2016-01-19T10:56:24Z DBG  output worker: publish 2 events<br />
2016-01-19T10:56:24Z DBG  Try to publish %!s(int=2) events to logstash with window size %!s(int=10)<br />
2016-01-19T10:56:24Z DBG  %!s(int=2) events out of %!s(int=2) events sent to logstash. Continue sending ...<br />
2016-01-19T10:56:24Z INFO Events sent: 2<br />
2016-01-19T10:56:24Z DBG  Processing 2 events<br />
2016-01-19T10:56:24Z DBG  Write registry file: /.filebeat<br />
2016-01-19T10:56:24Z INFO Registry file updated. 6 states written.<br />
...<br />
[/code]</p>
<p>The filebeat.log indicates that Filebeat daemon is sending the events to Logstash (ELK container).</p>
<p><strong>7) Viewing the raw log events from Kibana</strong></p>
<p>I am not filtering the log events, Logstash will be received the informations as it is.<br />
In a next blog post I will explain how to visualize the log events using filters, queries and graphs.</p>
<p>To view the raw log events, just open Kibana from a browser, in my case is in <code>http://192.168.99.100:5601</code>.<br />
Go to <code>Kibana &amp;gt; Settings</code>, add and create a new <code>Index Pattern</code> using the next:<br />
* Index name or pattern : <code>filebeat-*</code>   (instead of <code>logstash-*</code>)<br />
* Time-field name: <code>@timestamp</code></p>
<p>After that, go to <code>Kibana &amp;gt; Discover</code> and select the recently created  <code>filebeat-*</code> Index Pattern and You will see your logs/events.</p>
<p><em>Kibana - Viewing WSO2 and Wiremock raw log events</em><br />
<img src="{{ site.baseurl }}/assets/blog-chilcano-logs-wso2-docker-elk-rtail-4-kibana-esb-am-dss-wiremock-logs-discovery.png" alt="Kibana - Viewing WSO2 and Wiremock raw log events" title="Viewing WSO2 and Wiremock raw log events" /></p>
<p>In the above figure, a raw log event for WSO2 API Manager in JSON format is like as below:</p>
<p>[code lang=bash]<br />
{<br />
  &quot;_index&quot;: &quot;filebeat-2016.01.19&quot;,<br />
  &quot;_type&quot;: &quot;log&quot;,<br />
  &quot;_id&quot;: &quot;AVJZhXK2D7hOMrzuotcM&quot;,<br />
  &quot;_score&quot;: null,<br />
  &quot;_source&quot;: {<br />
    &quot;message&quot;: &quot;TID: [0] [AM] [2016-01-19 10:54:12,666]  INFO {org.wso2.carbon.core.internal.permission.update.PermissionUpdater} -  Permission cache updated for tenant -1234 {org.wso2.carbon.core.internal.permission.update.PermissionUpdater}&quot;,<br />
    &quot;@version&quot;: &quot;1&quot;,<br />
    &quot;@timestamp&quot;: &quot;2016-01-19T10:54:16.057Z&quot;,<br />
    &quot;beat&quot;: {<br />
      &quot;hostname&quot;: &quot;wso2-dev-srv-01&quot;,<br />
      &quot;name&quot;: &quot;wso2-dev-srv-01&quot;<br />
    },<br />
    &quot;count&quot;: 1,<br />
    &quot;fields&quot;: null,<br />
    &quot;input_type&quot;: &quot;log&quot;,<br />
    &quot;offset&quot;: 0,<br />
    &quot;source&quot;: &quot;/opt/wso2am02a/repository/logs/wso2carbon.log&quot;,<br />
    &quot;type&quot;: &quot;log&quot;,<br />
    &quot;host&quot;: &quot;wso2-dev-srv-01&quot;<br />
  },<br />
  &quot;fields&quot;: {<br />
    &quot;@timestamp&quot;: [<br />
      1453200856057<br />
    ]<br />
  },<br />
  &quot;sort&quot;: [<br />
    1453200856057<br />
  ]<br />
}<br />
[/code]</p>
<p>As I said above, in a next blog post I will explain how to visualize the log events using filters to parse simple event and multiple events.</p>
<p>That's all.<br />
I hope you enjoyed it.</p>
<p><strong>References:</strong><br />
* Vagrant box with WSO2 stack and Wiremock (https://github.com/Chilcano/box-vagrant-wso2-dev-srv)<br />
* Elasticsearch, Logstash, Kibana (ELK) Docker image by Sébastien Pujadas (https://github.com/spujadas/elk-docker)</p>
